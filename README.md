# ml_and_malware
Machine Learning and Malware
The following code was used for the analysis of several classifiers

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import (
    RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier
)
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    roc_curve
)
from sklearn.neighbors import NearestCentroid

# Fetch dataset from github user saurabh48782.
!curl --remote-name \
     -H 'Accept: application/vnd.github.v3.raw' \
     --location https://raw.githubusercontent.com/saurabh48782/Malware_Classification/master/MalwareData.csv

db = pd.read_csv('MalwareData.csv', delimiter="|")

pd.set_option('display.max_columns', None)
legitimate = db[db['legitimate'] == 1]
malware = db[db['legitimate'] == 0]

data_in_legitimate = legitimate.drop(["Name", "md5", "Machine", "SizeOfOptionalHeader", "MajorOperatingSystemVersion", "MinorOperatingSystemVersion", "LoaderFlags", "legitimate"], axis=1).values
data_in_malware = malware.drop(["Name", "md5", "Machine", "SizeOfOptionalHeader", "MajorOperatingSystemVersion", "MinorOperatingSystemVersion", "LoaderFlags", "legitimate"], axis=1).values

labels_legitimate = legitimate["legitimate"].values
labels_malware = malware["legitimate"].values

# Combine legitimate and malware data for stratified split
data_in_combined = np.concatenate((data_in_legitimate, data_in_malware), axis=0)
labels_combined = np.concatenate((labels_legitimate, labels_malware), axis=0)

# Perform stratified train-test split
train_data, test_data, train_labels, test_labels = train_test_split(data_in_combined, labels_combined, test_size=0.2, stratify=labels_combined, random_state=42)

classifiers = {
    "RandomForest": RandomForestClassifier(max_depth=5, n_estimators=53, max_features=1, random_state=42),
    "MLP": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=0.0001, solver='adam', random_state=42),
    "GaussianNB": GaussianNB(),
    "KNeighbors": KNeighborsClassifier(),
    "ExtraTree": ExtraTreeClassifier(random_state=42),
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "QDA": QuadraticDiscriminantAnalysis(),
    "PassiveAggressive": PassiveAggressiveClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "GradientBoosting": GradientBoostingClassifier(random_state=42),
    "Bagging": BaggingClassifier(random_state=42),
    "BernoulliNB": BernoulliNB(),
    "LinearDiscriminantAnalysis": LinearDiscriminantAnalysis(),
    "LinearSVC": LinearSVC(random_state=42),
    "MultinomialNB": MultinomialNB(),
    "NearestCentroid": NearestCentroid()
}

performance_metrics = {
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1 Score": [],
    "AUC": []
}

combined_confusion_matrix = np.zeros((2, 2), dtype=int)

def evaluate_model(name, model, train_data, train_labels, test_data, test_labels):
    model.fit(train_data, train_labels)
    predicted_labels = model.predict(test_data)
    
    accuracy = accuracy_score(test_labels, predicted_labels)
    precision = precision_score(test_labels, predicted_labels)
    recall = recall_score(test_labels, predicted_labels)
    f1 = f1_score(test_labels, predicted_labels, average="weighted")
    auc = roc_auc_score(test_labels, predicted_labels)
    
    print(f"{name} - Accuracy: {accuracy:.4f}")
    print(f"{name} - Precision: {precision:.4f}")
    print(f"{name} - Recall: {recall:.4f}")
    print(f"{name} - F1 Score: {f1:.4f}")
    print(f"{name} - AUC: {auc:.4f}")
    
    cm = confusion_matrix(test_labels, predicted_labels)
    global combined_confusion_matrix
    combined_confusion_matrix += cm
    
    ConfusionMatrixDisplay(confusion_matrix=cm).plot()
    
    performance_metrics["Accuracy"].append(accuracy)
    performance_metrics["Precision"].append(precision)
    performance_metrics["Recall"].append(recall)
    performance_metrics["F1 Score"].append(f1)
    performance_metrics["AUC"].append(auc)
    
    if hasattr(model, "decision_function"):
        y_scores = model.decision_function(test_data)
    elif hasattr(model, "predict_proba"):
        y_scores = model.predict_proba(test_data)[:, 1]
    else:
        y_scores = None
    
    return y_scores

fig, axs = plt.subplots(len(classifiers) + 1, 1, figsize=(10, len(classifiers) * 5))

# Scatter plot of the original data
axs[0].scatter(data_in_malware[:, 1], data_in_malware[:, 0], c=labels_malware, cmap='viridis', alpha=0.5)
axs[0].set_title('Original Malware Data')

# Evaluate all classifiers and plot results
for i, (name, classifier) in enumerate(classifiers.items()):
    print(f"{name} Performance:")
    y_scores = evaluate_model(name, classifier, train_data, train_labels, test_data, test_labels)
    
    # Scatter plot of the classifier results
    if y_scores is not None:
        axs[i + 1].scatter(test_data[:, 1], test_data[:, 0], c=y_scores, cmap='viridis', alpha=0.5)
    axs[i + 1].set_title(f'{name} Results')
    print()

plt.tight_layout()
plt.show()

# Plot combined ROC curves
plt.figure(figsize=(12, 8))
for name, classifier in classifiers.items():
    y_scores = evaluate_model(name, classifier, train_data, train_labels, test_data, test_labels)
    if y_scores is not None:
        fpr, tpr, _ = roc_curve(test_labels, y_scores)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(test_labels, y_scores):.4f})')

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.show()

# Display the combined confusion matrix
plt.figure(figsize=(8, 6))
ConfusionMatrixDisplay(confusion_matrix=combined_confusion_matrix).plot()
plt.title('Combined Confusion Matrix')
plt.show()

# Calculate and display variance and standard deviation of performance metrics
for metric, values in performance_metrics.items():
    mean = np.mean(values)
    std_dev = np.std(values)
    variance = np.var(values)
    print(f"\n{metric} - Mean: {mean:.4f}, Standard Deviation: {std_dev:.4f}, Variance: {variance:.4f}")
```
